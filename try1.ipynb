{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "import csv \n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm \n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec.load(\"SG_300_25_20/SG_300_25_20.model\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_tag = r\"<\\S*>\"\n",
    "invalid_characters = r\"[^a-zA-Z0-9 ]\"\n",
    "percentage = r\"[0-9]+%\"\n",
    "hashtag = r\"#\\S*\"\n",
    "numeric = r\"[0-9]+\"\n",
    "at = r\"@\\S*\"\n",
    "address = r\"\\S+@\\S+\\.\\S+\"\n",
    "link = r\"(https?:\\/\\/|www.)\\S+\"\n",
    "\n",
    "def preprocess(s):\n",
    "    # Remove html tags.\n",
    "    s = re.sub(html_tag, \" \", s)\n",
    "\n",
    "    # Substitute percents.\n",
    "    s = re.sub(percentage, \" procent \", s)\n",
    "\n",
    "    # Substitute hashtags.\n",
    "    s = re.sub(hashtag, \" hashtag \", s)\n",
    "\n",
    "    # Substitute at.\n",
    "    s = re.sub(at, \" entitate \", s)\n",
    "\n",
    "    # Substitute numbers. \n",
    "    s = re.sub(numeric, \" numar \", s)\n",
    "\n",
    "    # Substitute addresses.\n",
    "    s = re.sub(address, \" adresa \", s)\n",
    "\n",
    "    # Substitute links.\n",
    "    s = re.sub(link, \" link \", s)\n",
    "\n",
    "    # Remove accents, diacritics.\n",
    "    s = unidecode(s)\n",
    "\n",
    "    # Keep only these characters\n",
    "    s = re.sub(invalid_characters, \" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "romainian_stopwords = set(stopwords.words(\"romanian\"))\n",
    "\n",
    "def tokenize(s):\n",
    "    tokens = s.split()\n",
    "    tokens = list(filter(lambda x: x not in romainian_stopwords, tokens))\n",
    "    tokens = [s.lower() for s in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts a list of tokens to a vector.\n",
    "# Only look at the first num_tokens tokens.\n",
    "def vectorize(tokens, num_tokens=2):\n",
    "    vectors = []\n",
    "    for i in range(num_tokens):\n",
    "        if i >= len(tokens):\n",
    "            vectors.append(torch.zeros(word2vec.vector_size))\n",
    "        else:\n",
    "            if tokens[i] in word2vec:\n",
    "                vectors.append(torch.tensor(word2vec[tokens[i]])) \n",
    "            else:\n",
    "                vectors.append(torch.zeros(word2vec.vector_size))\n",
    "    return torch.cat(vectors, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train():\n",
    "    df = pd.read_csv('train.csv')\n",
    "    df = df.fillna(\"\")\n",
    "\n",
    "    contents =  [vectorize(tokenize(preprocess(x)), num_tokens=10) for x in tqdm(df[\"content\"])]\n",
    "    titles = [vectorize(tokenize(preprocess(x)), num_tokens=20) for x in tqdm(df[\"title\"])]\n",
    "    labels = list(df[\"class\"])\n",
    "\n",
    "    x = []\n",
    "    for i in range(len(contents)):\n",
    "        vec = torch.concat([contents[i], titles[i]], dim=0)\n",
    "        x.append(vec)\n",
    "\n",
    "    return x , labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70575/70575 [01:25<00:00, 828.52it/s] \n",
      "100%|██████████| 70575/70575 [00:15<00:00, 4587.85it/s]\n"
     ]
    }
   ],
   "source": [
    "x, labels = load_train() \n",
    "train_x, test_x, train_labels, test_labels = train_test_split(x, labels, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, labels):\n",
    "        self.x = x\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], 1 if self.labels[index] == True else 0\n",
    "\n",
    "NUM_FEATURES = train_x[0].shape[0]\n",
    "train_dataset = MyDataset(train_x, train_labels)\n",
    "test_dataset = MyDataset(test_x, test_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(NUM_FEATURES, 1000)\n",
    "        self.layer2 = nn.Linear(1000, 500)\n",
    "        self.layer3 = nn.Linear(500, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = self.forward(x)\n",
    "        return torch.argmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "model = Model()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    running_loss = 0\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in tqdm(enumerate(train_dataloader)):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        last_loss = loss.item()\n",
    "        running_loss += last_loss\n",
    "\n",
    "        if i % 40 == 39:\n",
    "            print(f\"Running loss: {running_loss/(i+1)}\") \n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    model.eval()\n",
    "\n",
    "    running_vloss = 0.0\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(test_dataloader):\n",
    "            x , labels = vdata\n",
    "            outputs = model(x)\n",
    "\n",
    "            vloss = loss_fn(outputs, labels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "            predictions = model.predict(x) \n",
    "            correct += torch.sum(predictions == labels)\n",
    "            total += len(labels)\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "\n",
    "    return avg_vloss, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:06,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.41797221079468727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:13,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.40581887178123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [00:20,  5.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.3994247309863567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "161it [00:28,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.39453000742942096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:35,  5.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.39151701793074606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "221it [00:38,  5.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3729833960533142, Test loss: 0.3874928653240204, Test accuracy: 0.9241940975189209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:07,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.369513414055109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:14,  5.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.3679558251053095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [00:21,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.36746755813558896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "161it [00:28,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.36616242192685605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:35,  5.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.36599480122327804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "221it [00:39,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3616737425327301, Test loss: 0.3788818418979645, Test accuracy: 0.9325540065765381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:07,  5.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.3527777068316936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:14,  6.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.35112944059073925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [00:21,  5.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.35110909764965376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "161it [00:28,  5.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.3521344779059291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:35,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.3532946555316448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "221it [00:38,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3522753119468689, Test loss: 0.36761027574539185, Test accuracy: 0.942472517490387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:07,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.3459581099450588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:14,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.34636778235435484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [00:22,  5.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.34733425204952556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "161it [00:30,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.3471878547221422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:37,  5.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.3472215610742569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "221it [00:41,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.36410456895828247, Test loss: 0.3654780685901642, Test accuracy: 0.945448100566864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:07,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.3402584508061409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:14,  5.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.34148963876068594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [00:22,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.34245903690656027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "161it [00:29,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.3419307816773653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:37,  5.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.3430040641129017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "221it [00:40,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.32656827569007874, Test loss: 0.36484000086784363, Test accuracy: 0.9471484422683716\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train(True)\n",
    "    train_loss = train_one_epoch()\n",
    "    test_loss, test_accuracy = test_model()\n",
    "\n",
    "    print(f\"Train loss: {train_loss}, Test loss: {test_loss}, Test accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test():\n",
    "    df = pd.read_csv('test.csv')\n",
    "    df = df.fillna(\"\")\n",
    "\n",
    "    contents =  [vectorize(tokenize(preprocess(x)), num_tokens=10) for x in tqdm(df[\"content\"])]\n",
    "    titles = [vectorize(tokenize(preprocess(x)), num_tokens=20) for x in tqdm(df[\"title\"])]\n",
    "\n",
    "    x = []\n",
    "    for i in range(len(contents)):\n",
    "        vec = torch.concat([contents[i], titles[i]], dim=0)\n",
    "        x.append(vec)\n",
    "\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36669/36669 [00:29<00:00, 1244.31it/s]\n",
      "100%|██████████| 36669/36669 [00:11<00:00, 3303.63it/s]\n"
     ]
    }
   ],
   "source": [
    "x_test = load_test() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36669/36669 [02:26<00:00, 249.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id  class\n",
      "0          0      0\n",
      "1          1      0\n",
      "2          2      0\n",
      "3          3      1\n",
      "4          4      1\n",
      "...      ...    ...\n",
      "36664  36664      0\n",
      "36665  36665      0\n",
      "36666  36666      0\n",
      "36667  36667      0\n",
      "36668  36668      0\n",
      "\n",
      "[36669 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ids = []\n",
    "predictions = []\n",
    "\n",
    "for i in tqdm(range(len(x_test))):\n",
    "    id = i\n",
    "    prediction = model.predict(torch.unsqueeze(x_test[i], 0)).item()\n",
    "\n",
    "    ids.append(id)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data={\"id\":ids, \"class\": predictions})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"1.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
