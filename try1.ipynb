{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "import csv \n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm \n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec.load(\"SG_300_25_20/SG_300_25_20.model\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_tag = r\"<\\S*>\"\n",
    "invalid_characters = r\"[^a-zA-Z0-9 ]\"\n",
    "percentage = r\"[0-9]+%\"\n",
    "hashtag = r\"#\\S*\"\n",
    "numeric = r\"[0-9]+\"\n",
    "at = r\"@\\S*\"\n",
    "address = r\"\\S+@\\S+\\.\\S+\"\n",
    "link = r\"(https?:\\/\\/|www.)\\S+\"\n",
    "\n",
    "def preprocess(s):\n",
    "    # Remove html tags.\n",
    "    s = re.sub(html_tag, \" \", s)\n",
    "\n",
    "    # Substitute percents.\n",
    "    s = re.sub(percentage, \" procent \", s)\n",
    "\n",
    "    # Substitute hashtags.\n",
    "    s = re.sub(hashtag, \" hashtag \", s)\n",
    "\n",
    "    # Substitute at.\n",
    "    s = re.sub(at, \" entitate \", s)\n",
    "\n",
    "    # Substitute numbers. \n",
    "    s = re.sub(numeric, \" numar \", s)\n",
    "\n",
    "    # Substitute addresses.\n",
    "    s = re.sub(address, \" adresa \", s)\n",
    "\n",
    "    # Substitute links.\n",
    "    s = re.sub(link, \" link \", s)\n",
    "\n",
    "    # Remove accents, diacritics.\n",
    "    s = unidecode(s)\n",
    "\n",
    "    # Keep only these characters\n",
    "    s = re.sub(invalid_characters, \" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "romainian_stopwords = set(stopwords.words(\"romanian\"))\n",
    "\n",
    "def tokenize(s):\n",
    "    tokens = s.split()\n",
    "    tokens = list(filter(lambda x: x not in romainian_stopwords, tokens))\n",
    "    tokens = [s.lower() for s in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts a list of tokens to a vector.\n",
    "# Only look at the first num_tokens tokens.\n",
    "def vectorize(tokens, num_tokens=2):\n",
    "    vectors = []\n",
    "    for i in range(num_tokens):\n",
    "        if i >= len(tokens):\n",
    "            vectors.append(torch.zeros(word2vec.vector_size))\n",
    "        else:\n",
    "            if tokens[i] in word2vec:\n",
    "                vectors.append(torch.tensor(word2vec[tokens[i]])) \n",
    "            else:\n",
    "                vectors.append(torch.zeros(word2vec.vector_size))\n",
    "    return torch.cat(vectors, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train():\n",
    "    df = pd.read_csv('train.csv')\n",
    "    df = df.fillna(\"\")\n",
    "\n",
    "    contents =  [vectorize(tokenize(preprocess(x)), num_tokens=10) for x in tqdm(df[\"content\"])]\n",
    "    titles = [vectorize(tokenize(preprocess(x)), num_tokens=20) for x in tqdm(df[\"title\"])]\n",
    "    labels = list(df[\"class\"])\n",
    "\n",
    "    x = []\n",
    "    for i in range(len(contents)):\n",
    "        vec = torch.concat([contents[i], titles[i]], dim=0)\n",
    "        x.append(vec)\n",
    "\n",
    "    return x , labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, labels = load_train() \n",
    "train_x, test_x, train_labels, test_labels = train_test_split(x, labels, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, labels):\n",
    "        self.x = x\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], 1 if self.labels[index] == True else 0\n",
    "\n",
    "NUM_FEATURES = train_x[0].shape[0]\n",
    "train_dataset = MyDataset(train_x, train_labels)\n",
    "test_dataset = MyDataset(test_x, test_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(NUM_FEATURES, 1000)\n",
    "        self.layer2 = nn.Linear(1000, 500)\n",
    "        self.layer3 = nn.Linear(500, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = self.forward(x)\n",
    "        return torch.argmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(weight=torch.tensor([3, 1.5]))\n",
    "model = Model()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    model.train(True)\n",
    "    running_loss = 0\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in tqdm(enumerate(train_dataloader)):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 40 == 39:\n",
    "            print(f\"Running loss: {running_loss/(i+1)}\") \n",
    "\n",
    "    return running_loss/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        running_vloss = 0.0\n",
    "\n",
    "        for i, vdata in enumerate(test_dataloader):\n",
    "            x , labels = vdata\n",
    "            outputs = model(x)\n",
    "\n",
    "            vloss = loss_fn(outputs, labels)\n",
    "            running_vloss += vloss.item()\n",
    "\n",
    "            current_predictions = model.predict(x) \n",
    "            predictions+=current_predictions\n",
    "            targets += labels\n",
    "\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "\n",
    "        return avg_vloss, balanced_accuracy_score(targets, predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30 \n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_one_epoch()\n",
    "    test_loss, test_accuracy = test_model()\n",
    "\n",
    "    print(f\"Train loss: {train_loss}, Test loss: {test_loss}, Test accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test():\n",
    "    df = pd.read_csv('test.csv')\n",
    "    df = df.fillna(\"\")\n",
    "\n",
    "    contents =  [vectorize(tokenize(preprocess(x)), num_tokens=10) for x in tqdm(df[\"content\"])]\n",
    "    titles = [vectorize(tokenize(preprocess(x)), num_tokens=20) for x in tqdm(df[\"title\"])]\n",
    "\n",
    "    x = []\n",
    "    for i in range(len(contents)):\n",
    "        vec = torch.concat([contents[i], titles[i]], dim=0)\n",
    "        x.append(vec)\n",
    "\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = load_test() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ids = []\n",
    "predictions = []\n",
    "\n",
    "for i in tqdm(range(len(x_test))):\n",
    "    id = i\n",
    "    prediction = model.predict(torch.unsqueeze(x_test[i], 0)).item()\n",
    "\n",
    "    ids.append(id)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data={\"id\":ids, \"class\": predictions})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"1.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
